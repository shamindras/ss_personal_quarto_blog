---
title: "Characterizing norm triangle inequalites via convexity"
description: |
  A simple approach to proving the triangle inequality for a given _norm-like_ function using convexity.
categories: [linear algebra, math, norms]
slug: shrotriya2022normconvexity
date: "2022-02-12"
image: images/preview-shrotriya2021normtriconvexity.png
bibliography: ../../refs.bib
format: 
  html:
    code-link: true
execute:
  echo: true
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup_01_01, include=FALSE, echo=FALSE, results='asis'}
wikipediapreview::wp_init(use_unpkg = TRUE, use_alt_style = TRUE)
```

```{r packages, include=FALSE}
library(bookdown)
library(emo)
```

# TL;DR

I walkthrough a cool and possibly less known result connecting convexity and
the triangle inequalities for norms. Using this result, typical proofs of the
triangle inequality for a proposed norm function are significantly simplified.
This exposition is based on [Chapter 3, @robinson2020introfuncanalysis_book]
[^1].

[^1]: **Note:** The presentation in this post is ***intentionally*** verbose.
    The goal is to give lots of intuition of the key result and its usefulness,
    and ensure that the proofs are rigorous. It is written with an empathetic
    mindset to newcomers, and to myself for future reference.

<aside>

[@robinson2020introfuncanalysis_book] is a wonderful functional analysis book,
highly recommend it `r emo::ji("100")`!

</aside>

# Background - Norms

<aside>

<br> <br> Experienced readers can freely skip this Background section.

</aside>

Normed linear spaces are a natural setting for much applied mathematics and
statistics. These are vector spaces, $V$, endowed with a norm function,
$\lVert \cdot \rVert_{V}$. Intuitively, norms give us a "yardstick" to measure
the "lengths" of individual vectors in the given vector space space. A standard
definition of a norm is as follows:

::: {#def-norm}

## Norms in vector spaces

For a given vector space $V$, a norm
$\lVert \cdot \rVert_{V}: V \to \mathbb{R}$, is a function satisfying the
following three properties.

1.  **Positive definiteness:** For all $\mathbf{x} \in V$, if
    $\lVert \mathbf{x} \rVert = 0$ then $\mathbf{x} = \mathbf{0}_{V}$.
2.  **Absolute homogeneity:**
    $\lVert \lambda \mathbf{x} \rVert = | \lambda | \lVert  \mathbf{x} \rVert$,
    for all $\mathbf{x} \in V, \lambda \in \mathbb{R}$.
3.  **Triangle inequality:**
    $\lVert \mathbf{x} + \mathbf{y} \rVert \leq \lVert \mathbf{x} \rVert +  \lVert \mathbf{y} \rVert$,
    for all $\mathbf{x}, \mathbf{y} \in V$.
:::

<aside>

We often use many norms on a given vector space, depending on which seems
"meaningful" for the given purpose. See
[here](https://en.wikipedia.org/wiki/Norm_(mathematics)#Examples){.wiki} for common examples.

</aside>

<details>

<summary>

Remarks

</summary>

::: {.remark name="Derived properties from @def-norm"}
We note that a norm, per @def-norm, in fact, **implies** the
following properties:

1.  In @def-norm, we can always replace **positive
    definiteness** with the stronger claim, namely that \begin{equation}
     \text{For all $\mathbf{x} \in V$, if $\lVert \mathbf{x} \rVert = 0 \iff \mathbf{x} = \mathbf{0}_{V}$.}
    \end{equation} In short, we want to show that the reverse implication to
    **positive definiteness** always holds, i.e.,
    $\mathbf{x} = \mathbf{0}_{V} \implies \lVert \mathbf{x} \rVert = 0$. To
    prove this observe that using **absolute homogeneity** in Definition
    \@def-norm, we have: \begin{equation}
     \lVert \mathbf{x} \rVert
     = \lVert \mathbf{0}_{V} \rVert
     = \lVert 0 (\mathbf{0}_{V}) \rVert
     = |0| \lVert \mathbf{0}_{V} \rVert = 0
    \end{equation} As required.

2.  We also have that $\lVert \mathbf{x} \rVert \geq 0$, for all
    $\mathbf{x} \in V$. To see this, observe that for all $\mathbf{x} \in V$
    \begin{equation}
    \begin{split}
     0
     & = \lVert \mathbf{0}_{V} \rVert
     \quad\text{(by previous part of this remark)}\\
     & = \lVert \mathbf{x} + (-\mathbf{x}) \rVert \\
     &\leq \lVert \mathbf{x} \rVert + \lVert -\mathbf{x} \rVert
     \quad\text{(by the triangle inequality)} \\
     &= \lVert \mathbf{x} \rVert +
        \lvert -1 \rvert \lVert \mathbf{x} \rVert
        \quad\text{(by absolute homogeneity)} \\
     &= 2 \lVert \mathbf{x} \rVert \\
    \implies \lVert \mathbf{x} \rVert
     & \geq 0
    \end{split}
    \end{equation} In effect this means the co-domain can *always* be changed
    from $\lVert \cdot \rVert_{V}: V \to \mathbb{R}$ to
    $\lVert \cdot \rVert_{V}: V \to \mathbb{R}_{\geq 0}$.

3.  Since these can always be derived directly from @def-norm,
    as shown, we can keep @def-norm in its minimal form as
    noted here.

4.  These ideas work for *seminorms* as well, see
    [here](https://en.wikipedia.org/wiki/Seminorm#Definition){.wiki} for more
    details.
:::

</details>

# Main theorem

<aside>

<br> <br> This is based on [Lemmas 3.3-3.4 @robinson2020introfuncanalysis_book].

</aside>

::: {#thm-normtrichar}

## Characterization of norm triangle inequality

Let $N: V \to \mathbb{R_{\geq 0}}$, be a function satisfying the following two
properties[^2].

1.  **Positive definiteness:** For all $\mathbf{x} \in V$, if
    $N(\mathbf{x}) = 0$ then $\mathbf{x} = \mathbf{0}_{V}$.
2.  **Absolute homogeneity:**
    $N(\lambda \mathbf{x}) = | \lambda | N(\mathbf{x})$, for all
    $\mathbf{x} \in V, \lambda \in \mathbb{R}$.

We then have that: 
$$
    N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})
    \text{, for each } \mathbf{x}, \mathbf{y} \in V
    \iff \mathbb{B} := \{\mathbf{z} \in V \:|\: N(\mathbf{z}) \leq 1 \}
    \text{ is convex}
$$ {#eq-normtrichar-01}
:::

[^2]: We refer to such an $N: V \to \mathbb{R_{\geq 0}}$ satisfying these
    properties, as a *norm-like* function.

In simple terms, the importance of @thm-normtrichar (as captured
by @eq-normtrichar-01) can be summarized as follows:

> Let $N : V \to [0, \infty)$ be a function satisfying positive definiteness and
> absolute homogeneity. Then $N$ satisfies the triangle inequality if and only
> if the unit ball induced by $N$, i.e.,
> $\mathbb{B} := \{\mathbf{z} \in V \:|\: N(\mathbf{z}) \leq 1 \}$, is a convex
> set.

<details>

<summary>

Remarks

</summary>

::: remark
In @thm-normtrichar, we note the following:

1.  The function $N : V \to \mathbb{R}_{\geq 0}$, is a *norm-like* function, and
    only becomes a valid norm per @def-norm ***once*** we
    establish the triangle inequality, i.e.,
    $N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})$.
2.  To prove the triangle inequality for $N : V \to \mathbb{R}_{\geq 0}$, the
    necessary condition of @thm-normtrichar to establish is:
    $$
     \mathbb{B} := \{\mathbf{z} \in V \:|\: N(\mathbf{z}) \leq 1 \}
     \text{ is convex}
    $${#eq-normtrichar-02} which will imply the triangle inequality for $N$ - huzzah!
3.  The nice thing is, proving the convexity of $\mathbb{B}$ can be *much
    easier* to show than trying to prove the triangle inequality property of $N$
    directly, as we will soon see.
4.  **Subtle point:** note that here we had to *assume* that the co-domain of
    $N$ is non-negative (not $\mathbb{R}$), i.e.,
    $N : V \to \mathbb{R}_{\geq 0}$. This is because in a typical norm, which
    satisfies the triangle inequality, is always shown to be non-negative (see
    remark below @def-norm for more details). Here we *impose*
    non-negativity of $N$ as an additional constraint to *establish* the
    triangle inequality property for $N$. This is not an issue, since one would
    always first check the non-negativity of a candidate *norm-like* function
    $N$.
:::

</details>

# Applications: Minkowski inequalities

Before getting into the details of the proof, let's just see what Theorem
@thm-normtrichar can do! We'll consider two related applications taken
from [Lemma 3.6, Example 3.13 @robinson2020introfuncanalysis_book],
respectively.

### Application 1: $\ell_{p}$-norm triangle inequality in $\mathbb{F}^{n}$

::: {#exm-normtrilpsum}

## Minkowski inequality in finite dimensions

Let us consider $(\mathbb{F}^{n}, \mathbb{F})$, where
$\mathbb{F} = \mathbb{R} \text{ or } \mathbb{C}$. We then define the *norm-like*
function $N_{\ell^{p}}: \mathbb{F}^{n} \to \mathbb{R}_{\geq 0}$:
$$
    N_{\ell^{p}}(\mathbf{x})
    :=\left(\sum_{j=1}^{n}\left|x_{j}\right|^{p}\right)^{1 / p}
    , \quad 1 \leq p<\infty
$${#eq-normtrilpsum-01}
:::

One can show that $N_{\ell^{p}}$ satisfies positive definiteness and absolute
homogeneity. To show that $N_{\ell^{p}}$ is a norm function we need to prove the
triangle inequality. We will use @thm-normtrichar. Let us define
$\mathbb{B} := \{\mathbf{z} \in \mathbb{F}^{n} \:|\: N_{\ell^{p}}(\mathbf{z}) \leq 1 \} = \{\mathbf{z} \in \mathbb{F}^{n} \:|\: N_{\ell^{p}}^{p}(\mathbf{z}) \leq 1 \}$.
We now need to show that $\mathbb{B}$ is convex. We will need to use the fact
that for each $t \in \mathbb{R}, t \mapsto |t|^{p}$ is convex. Let
$\mathbf{x}, \mathbf{y} \in \mathbb{B}$, we then have that for
$\lambda \in [0, 1]$:

<aside>

**Note:** $t \mapsto |t|^{p}$, is convex, for each $t \in \mathbb{R}$ and
$p \in [1, \infty)$. **Sketch:** We have, $|t|^{p} =: h(t) := (g \circ f) (t)$.
Here, for each $x \in \mathbb{R}$, we observe $g : x \mapsto x^{p}$ is
increasing and convex, and $f : x \mapsto |x|$ is convex. Thus,
$h(t) := |t|^{p}$ is convex. $\blacksquare$

</aside>

```{=tex}
\begin{equation}
\begin{split}
    N_{\ell^{p}}^{p}(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
    & = \sum_{j=1}^{n}|\lambda| x_{j}|+(1-\lambda)| y_{j}||^{p}
    \quad\text{(by definition)} \\
    & \leq \sum_{j=1}^{n} \lambda\left|x_{j}\right|^{p}+(1-\lambda)\left|y_{j}\right|^{p}
    \quad\text{(since $t \mapsto |t|^{p}$ is convex for each $t \in \mathbb{R}$)} \\
    & = \lambda \sum_{j=1}^{n} \left|x_{j}\right|^{p}
    + (1 - \lambda) \sum_{j=1}^{n} \left|y_{j}\right|^{p} \\
    & \leq 1
    \quad\text{(since $\mathbf{x}, \mathbf{y} \in \mathbb{B}$.)}
\end{split}
\end{equation}
```
It follows that
$N_{\ell^{p}}(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y}) \leq 1$, and so
$\lambda \mathbf{x} + (1 - \lambda) \mathbf{y} \in \mathbb{B}$, as required
$\blacksquare$.

<aside>

This triangle inequality is proved in just four lines `r emo::ji("fire")`!

</aside>

In fact, since it $N_{\ell^{p}}$ satisfies the three conditions for a norm per
@def-norm we can now denote it using the conventional
$\ell_{p}$-norm form, i.e.,
$\| \mathbf{x} \|_{\ell^{p}} := N_{\ell^{p}}(\mathbf{x})$

### Application 2: $L_{p}$-norm triangle inequality

We can also similarly prove the triangle inequality norms involving integrals
efficiently. This is seen in the next example.

<aside>

Here $C([0, 1])$ is the space of continuous functions on the interval $[0, 1]$.

</aside>

::: {#normtrilpint .example name="Minkowski inequality in integral form"}
Let us consider $(C([0, 1]), \mathbb{R})$. We then define the *norm-like*
function $N_{L^{p}}: C([0, 1]) \to \mathbb{R}_{\geq 0}$: 
$$
    N_{L^{p}}(\mathbf{x})
    :=\left(\int_{0}^{1}\left|f(x)\right|^{p}\right)^{1 / p}
    , \quad 1 \leq p<\infty
$${#eq-normtrilpint-01}
:::

Let us define
$\mathbb{B} := \{h \in C([0, 1]) \:|\: N_{L^{p}}(h) \leq 1 \}  = \{h \in C([0, 1]) \:|\: N_{L^{p}}^{p}(h) \leq 1 \}$.
We now need to show that $\mathbb{B}$ is convex. Let $f, g \in \mathbb{B}$, we
then have that for $\lambda \in [0, 1]$:

<aside>

Once again, one can show that $N_{\ell^{p}}$ satisfies positive definiteness and
absolute homogeneity.

</aside>

```{=tex}
\begin{equation}
\begin{split}
    N_{L^{p}}^{p}(\lambda f + (1 - \lambda) g)
    & = \int_{0}^{1}|\lambda f(x) + (1-\lambda) g(x)|^{p} dx
    \quad\text{(by definition)} \\
    & \leq \int_{0}^{1}\lambda |f(x)|^{p} + (1-\lambda) |g(x)|^{p} dx
    \quad\text{(since $t \mapsto |t|^{p}$ is convex for each $t \in \mathbb{R}$)} \\
    & = \lambda \int_{0}^{1} |f(x)|^{p} dx + (1-\lambda) \int_{0}^{1} |g(x)|^{p} dx \\
    & \leq 1
    \quad\text{(since $f, g \in \mathbb{B}$.)}
\end{split}
\end{equation}
```
It follows that $N_{L^{p}}(\lambda f + (1 - \lambda) g) \leq 1$, and so
$\lambda f + (1 - \lambda) g \in \mathbb{B}$, as required $\blacksquare$.

Again, we can now denote $N_{L^{p}}$ using the conventional $L_{p}$-norm form,
i.e., $\| f \|_{L^{p}} := N_{L^{p}}(f)$.

# Punchline: what did Theorem 1 buy us?

We just saw that applying @thm-normtrichar enabled us to write **very short
proofs** of [**Minkowski's
inequality**](https://en.wikipedia.org/wiki/Minkowski_inequality){.wiki} in
$\mathbb{F}^{n}$ and $C([0, 1])$.

To appreciate this approach, note that proving Minkowski's inequality typically
requires one to first prove [**Young's
inequality**](https://en.wikipedia.org/wiki/Young%27s_inequality_for_products){.wiki} and then
[**Hölder's
inequality**](https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality){.wiki}. Moreover these need to be done separately in
$\mathbb{F}^{n}$ and $C([0, 1])$. Using @thm-normtrichar allowed
us to achieve both of these goals using near identical style of proofs
`r emo::ji("tada")`!

<aside>

**Note:** The proofs of Young's and Hölder's inequality are beautiful and
studying them is also insightful.

</aside>

# Proof of Theorem 1

Assuming $N: V \to \mathbb{R_{\geq 0}}$ satisfies the two properties in Theorem
@thm-normtrichar, we need to prove both implications in Equation
@eq-normtrichar-01.

### Proof - easy direction

Assume that $N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})$, for
each $\mathbf{x}, \mathbf{y} \in V$. Let $\lambda \in [0, 1]$ be arbitrary. We
need to show that this implies for each $\mathbf{x}, \mathbf{y} \in \mathbb{B}$
that the expression
$\lambda \mathbf{x} + (1 - \lambda) \mathbf{y} \in \mathbb{B}$ holds. This
implies the convexity of $\mathbb{B}$.

<details>

<summary>

Proof

</summary>

::: {#normtrichar-easy .proof}
($\implies$) We proceed directly.

Assume that $N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})$, for
each $\mathbf{x}, \mathbf{y} \in V$. Let $\lambda \in [0, 1]$ be arbitrary. We
need to show that this implies for each $\mathbf{x}, \mathbf{y} \in \mathbb{B}$
that the expression
$\lambda \mathbf{x} + (1 - \lambda) \mathbf{y} \in \mathbb{B}$ holds. This
implies the convexity of $\mathbb{B}$.

We observe that for $\lambda \in \{0, 1\}$ our required expression is equal to
either $\mathbf{x}$ or $\mathbf{y}$ which are both in $\mathbb{B}$, by
assumption. Now fix $\lambda \in (0, 1)$. We then note:

```{=tex}
\begin{equation}
\begin{split}
    N(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
    & \leq N(\lambda \mathbf{x}) + N((1 - \lambda) \mathbf{y})
    \quad\text{($N$ satisfies triangle inequality)} \\
    & = \lvert \lambda \rvert N(\mathbf{x}) + \lvert 1 - \lambda \rvert N(\mathbf{y})
    \quad\text{(by absolute homogeneity of $N$)} \\
    & = \lambda N(\mathbf{x}) + 1 - \lambda N(\mathbf{y})
    \quad\text{(since $\lambda > 0$)} \\
    & \leq (\lambda) (1) + (1 - \lambda) (1)
    \quad\text{(since $N(\mathbf{z}) \leq 1$, for $\mathbf{z} \in \mathbb{B}$)} \\
    & = 1 \\
\implies \lambda \mathbf{x} + (1 - \lambda) \mathbf{y}
    & \in \mathbb{B}
\end{split}
\end{equation}
```
Which implies the convexity of $\mathbb{B}$, as required. $\blacksquare$
:::

</details>

### Proof - interesting direction

Assume $\mathbb{B}$ is a convex set. We need to show that this implies that
$N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})$, for each
$\mathbf{x}, \mathbf{y} \in V$.

<aside>

**Tip:** In linear algebra proofs, it is best to **first** deal with the zero
vector, $\mathbf{0}_{V}$, separately.

</aside>

<details>

<summary>

Proof

</summary>

::: {#normtrichar-hard .proof}
($\impliedby$) We proceed directly.

Assume $\mathbb{B}$ is a convex set. We need to show that this implies that
$N(\mathbf{x} + \mathbf{y}) \leq N(\mathbf{x}) + N(\mathbf{y})$, for each
$\mathbf{x}, \mathbf{y} \in V$.

Let $\mathbf{x}, \mathbf{y} \in V$. We will consider four cases.

**Case 1:** Let $\mathbf{x} = \mathbf{y} = \mathbf{0}_{V}$. Then
$N(\mathbf{x}) = N(\mathbf{y}) = N(\mathbf{0}_{V}) = N(0 \mathbf{0}_{V})  = |0| N(\mathbf{0}_{V})= 0$,
by absolute homogeneity of $N$. Indeed we then have that
$N(\mathbf{x} + \mathbf{y}) = N(\mathbf{0}_{V}) = 0 = N(\mathbf{x}) + N(\mathbf{y})$,
as required.

**Case 2:** Let
$\mathbf{x} = \mathbf{0}_{V}, \mathbf{y} \in V \setminus \{\mathbf{0}_{V}\}$.
Then $N(\mathbf{y}) = 0$, and it follows that
$N(\mathbf{x} + \mathbf{y}) = N(\mathbf{x} + \mathbf{0}_{V}) = N(\mathbf{x})  = N(\mathbf{x}) + 0 = N(\mathbf{x}) + N(\mathbf{y})$,
as required.

**Case 3:** Let
$\mathbf{x} \in V \setminus \{\mathbf{0}_{V}\}, \mathbf{y} = \mathbf{0}_{V}$.
Same as **Case 2**, with the roles of $\mathbf{x}, \mathbf{y}$ reversed.

**Case 4:** Let $\mathbf{x}, \mathbf{y} \in V \setminus \{\mathbf{0}_{V}\}$. It
then follows that $N(\mathbf{x}), N(\mathbf{y}) > 0$, since
$N(\mathbf{z}) \geq 0$, for each $\mathbf{z} \in V$, and
$N(\mathbf{z}) = 0 \iff \mathbf{z} = \mathbf{0}_{V}$. Moreover we then have that
$\lvert N(\mathbf{x}) \rvert = N(\mathbf{x}) > 0$ and
$\lvert N(\mathbf{y}) \rvert = N(\mathbf{y}) > 0$. So we can safely divide by
these quantities. Let us then define
$\tilde{\mathbf{x}} := \frac{\mathbf{x}}{N(\mathbf{x})}, \tilde{\mathbf{y}} := \frac{\mathbf{y}}{N(\mathbf{y})}$.
We then have by absolute homogeneity of $N$ that,
$N(\tilde{\mathbf{x}}) := N\left(\frac{\mathbf{x}}{N(\mathbf{x})}\right) = \left \lvert \frac{1}{N(\mathbf{x})} \right \rvert N(\mathbf{x}) = 1 \implies \tilde{\mathbf{x}} \in \mathbb{B}$.
Similarly $\tilde{\mathbf{y}} \in \mathbb{B}$. Let us denote
$\lambda := \frac{N(\mathbf{x})}{N(\mathbf{x}) + N(\mathbf{y})} \in (0, 1)$, and
$\mathbf{z} := \frac{\mathbf{x} + \mathbf{y}}{N(\mathbf{x}) + N(\mathbf{y})}$.
We then have: \begin{equation}
\begin{split}
    \mathbf{z}
    & := \frac{\mathbf{x} + \mathbf{y}}{N(\mathbf{x}) + N(\mathbf{y})} \\
    & = \left(\frac{N(\mathbf{x})}{N(\mathbf{x}) + N(\mathbf{y})}\right)
        \left(\frac{\mathbf{x}}{N(\mathbf{x})}\right) +
        \left(\frac{N(\mathbf{y})}{N(\mathbf{x}) + N(\mathbf{y})}\right)
        \left(\frac{\mathbf{y}}{N(\mathbf{y})}\right) \\
    & = \left(\frac{N(\mathbf{x})}{N(\mathbf{x}) + N(\mathbf{y})}\right)
        \tilde{\mathbf{x}} +
        \left(\frac{N(\mathbf{y})}{N(\mathbf{x}) + N(\mathbf{y})}\right)
        \tilde{\mathbf{y}} \\
    & = \lambda \tilde{\mathbf{x}} +
        (1 - \lambda) \tilde{\mathbf{y}} \\
    & \in \mathbb{B}
\end{split}
\end{equation}

By the assumed convexity of $\mathbb{B}$. We then have that
$\mathbf{z} := \frac{\mathbf{x} + \mathbf{y}}{N(\mathbf{x}) + N(\mathbf{y})}  \in \mathbb{B} \implies N(\mathbf{z}) \leq 1$.
We then observe: \begin{equation}
\begin{split}
    N(\mathbf{z})
    & \leq 1
    \quad\text{(since $\mathbf{z} \in \mathbb{B}$.)} \\
\iff N\left( \frac{\mathbf{x} + \mathbf{y}}{N(\mathbf{x}) + N(\mathbf{y})}\right)
    & \leq 1
    \quad\text{(by definition of $\mathbf{z}$.)} \\
\iff \left\lvert \frac{1}{N(\mathbf{x}) + N(\mathbf{y})}\right\rvert
     N(\mathbf{x} + \mathbf{y})
    & \leq 1
    \quad\text{(absolute homogeneity of $N$.)} \\
\iff \frac{1}{N(\mathbf{x}) + N(\mathbf{y})}
     N(\mathbf{x} + \mathbf{y})
    & \leq 1
    \quad\text{(since $N(\mathbf{x}), N(\mathbf{y}) > 0$.)} \\
\iff N(\mathbf{x} + \mathbf{y})
    & \leq N(\mathbf{x}) + N(\mathbf{y})
\end{split}
\end{equation}

As required. $\blacksquare$
:::

</details>

# Recap

In this article we learned the following about @thm-normtrichar:

-   It gives an alternative way to **characterize** the triangle inequality for
    *norm-like* functions.
-   Using this characterization **we can prove the triangle inequality** for
    such norm-like functions using the convexity of the unit ball induced by
    such functions.
-   This is usually easier since we have lots of **tools from convex analysis**
    to help us prove the convexity of $\mathbb{B}$.
-   We saw this in action since @thm-normtrichar enabled us to
    write **very short proofs** of **Minkowski's inequality** in
    $\mathbb{F}^{n}$ and $C([0, 1])$.

In summary, if you have a norm-like function for which you are trying to
establish the triangle inequality, try out @thm-normtrichar
`r emo::ji("100")`!

# Acknowledgements {.appendix}

I thank [Prof. James
Robinson](https://warwick.ac.uk/fac/sci/maths/people/staff/james_robinson) for
providing several technical clarifications on @thm-normtrichar. I thank [Mikhail
Popov](https://mpopov.com/about/) for creating the [`wikipediapreview` R
package](https://bearloga.github.io/wikipediapreview-r/), which enable an easy
interface for Wikipedia [Context Cards](https://chimeces.com/context-cards/) in
`Rmd` files. These Context Cards enable the hover over preview for Wikipedia
articles. I thank [Jewel Johnson](https://jeweljohnsonj.github.io/jewel_resume/)
for providing [this helpful
guide](https://jeweljohnsonj.github.io/jeweljohnson.github.io/posts/2021-12-18-quality-of-life-modifications-for-your-distill-webistes/#making-the-table-of-contents-more-useful)
to enable fixed TOC for this article. I thank [Dr. Joel
Nitta](https://www.joelnitta.com/) for providing [these
instructions](https://www.joelnitta.com/posts/2021-11-24_using-giscus/) to
enable us to switch to the [giscus](https://giscus.app/) comments system. Much
of these distill site improvements were brought to our attention due to the
excellent [distillery](https://distillery.rbind.io/tips_and_tricks.html) site
run by [Prof. John Paul Helveston](https://www.jhelvy.com/).
