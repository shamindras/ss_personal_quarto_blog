{
  "hash": "534fd9dbf509f821bc53a109d475aa57",
  "result": {
    "markdown": "---\ntitle: \"Characterizing norm triangle inequalites via convexity\"\ndescription: |\n  A simple approach to proving the triangle inequality for a given _norm-like_ function using convexity.\ncategories: [linear algebra, math, norms]\nslug: shrotriya2022normconvexity\ndate: \"2022-02-12\"\nimage: images/preview-shrotriya2021normtriconvexity.png\nbibliography: ../../refs.bib\nformat: \n  html:\n    code-link: true\nexecute:\n  echo: true\neditor_options: \n  markdown: \n    wrap: 80\n---\n\n\n\n\n\n\n<!-- Place fontawesome kit's code here -->\n\n\n```{=html}\n<script src=\"https://kit.fontawesome.com/a403fab802.js\" crossorigin=\"anonymous\"></script>\n```\n\n\n\n\n<!-- https://chimeces.com/context-cards/ -->\n\n<!-- Source: https://mpopov.com/wikipedia-context-cards-test/ -->\n\n\n```{=html}\n<script src=\"https://unpkg.com/context-cards/dist/context-cards.js\"></script>\n```\n\n# TL;DR\n\nI walkthrough a cool and possibly less known result connecting convexity and\nthe triangle inequalities for norms. Using this result, typical proofs of the\ntriangle inequality for a proposed norm function are significantly simplified.\nThis exposition is based on [Chapter 3, @robinson2020introfuncanalysis_book]\n[^1].\n\n[^1]: **Note:** The presentation in this post is ***intentionally*** verbose.\n    The goal is to give lots of intuition of the key result and its usefulness,\n    and ensure that the proofs are rigorous. It is written with an empathetic\n    mindset to newcomers, and to myself for future reference.\n\n<aside>\n\n[@robinson2020introfuncanalysis_book] is a wonderful functional analysis book,\nhighly recommend it ðŸ’¯!\n\n</aside>\n\n# Background - Norms\n\n<aside>\n\n<br> <br> Experienced readers can freely skip this Background section.\n\n</aside>\n\nNormed linear spaces are a natural setting for much applied mathematics and\nstatistics. These are vector spaces, $V$, endowed with a norm function,\n$\\lVert \\cdot \\rVert_{V}$. Intuitively, norms give us a \"yardstick\" to measure\nthe \"lengths\" of individual vectors in the given vector space space. A standard\ndefinition of a norm is as follows:\n\n::: {#def-norm}\n\n## Norms in vector spaces\n\nFor a given vector space $V$, a norm\n$\\lVert \\cdot \\rVert_{V}: V \\to \\mathbb{R}$, is a function satisfying the\nfollowing three properties.\n\n1.  **Positive definiteness:** For all $\\mathbf{x} \\in V$, if\n    $\\lVert \\mathbf{x} \\rVert = 0$ then $\\mathbf{x} = \\mathbf{0}_{V}$.\n2.  **Absolute homogeneity:**\n    $\\lVert \\lambda \\mathbf{x} \\rVert = | \\lambda | \\lVert  \\mathbf{x} \\rVert$,\n    for all $\\mathbf{x} \\in V, \\lambda \\in \\mathbb{R}$.\n3.  **Triangle inequality:**\n    $\\lVert \\mathbf{x} + \\mathbf{y} \\rVert \\leq \\lVert \\mathbf{x} \\rVert +  \\lVert \\mathbf{y} \\rVert$,\n    for all $\\mathbf{x}, \\mathbf{y} \\in V$.\n:::\n\n<aside>\n\nWe often use many norms on a given vector space, depending on which seems\n\"meaningful\" for the given purpose. See\n[here](https://en.wikipedia.org/wiki/Norm_(mathematics)#Examples){.wiki} for common examples.\n\n</aside>\n\n<details>\n\n<summary>\n\nRemarks\n\n</summary>\n\n::: {.remark name=\"Derived properties from @def-norm\"}\nWe note that a norm, per @def-norm, in fact, **implies** the\nfollowing properties:\n\n1.  In @def-norm, we can always replace **positive\n    definiteness** with the stronger claim, namely that \\begin{equation}\n     \\text{For all $\\mathbf{x} \\in V$, if $\\lVert \\mathbf{x} \\rVert = 0 \\iff \\mathbf{x} = \\mathbf{0}_{V}$.}\n    \\end{equation} In short, we want to show that the reverse implication to\n    **positive definiteness** always holds, i.e.,\n    $\\mathbf{x} = \\mathbf{0}_{V} \\implies \\lVert \\mathbf{x} \\rVert = 0$. To\n    prove this observe that using **absolute homogeneity** in Definition\n    \\@def-norm, we have: \\begin{equation}\n     \\lVert \\mathbf{x} \\rVert\n     = \\lVert \\mathbf{0}_{V} \\rVert\n     = \\lVert 0 (\\mathbf{0}_{V}) \\rVert\n     = |0| \\lVert \\mathbf{0}_{V} \\rVert = 0\n    \\end{equation} As required.\n\n2.  We also have that $\\lVert \\mathbf{x} \\rVert \\geq 0$, for all\n    $\\mathbf{x} \\in V$. To see this, observe that for all $\\mathbf{x} \\in V$\n    \\begin{equation}\n    \\begin{split}\n     0\n     & = \\lVert \\mathbf{0}_{V} \\rVert\n     \\quad\\text{(by previous part of this remark)}\\\\\n     & = \\lVert \\mathbf{x} + (-\\mathbf{x}) \\rVert \\\\\n     &\\leq \\lVert \\mathbf{x} \\rVert + \\lVert -\\mathbf{x} \\rVert\n     \\quad\\text{(by the triangle inequality)} \\\\\n     &= \\lVert \\mathbf{x} \\rVert +\n        \\lvert -1 \\rvert \\lVert \\mathbf{x} \\rVert\n        \\quad\\text{(by absolute homogeneity)} \\\\\n     &= 2 \\lVert \\mathbf{x} \\rVert \\\\\n    \\implies \\lVert \\mathbf{x} \\rVert\n     & \\geq 0\n    \\end{split}\n    \\end{equation} In effect this means the co-domain can *always* be changed\n    from $\\lVert \\cdot \\rVert_{V}: V \\to \\mathbb{R}$ to\n    $\\lVert \\cdot \\rVert_{V}: V \\to \\mathbb{R}_{\\geq 0}$.\n\n3.  Since these can always be derived directly from @def-norm,\n    as shown, we can keep @def-norm in its minimal form as\n    noted here.\n\n4.  These ideas work for *seminorms* as well, see\n    [here](https://en.wikipedia.org/wiki/Seminorm#Definition){.wiki} for more\n    details.\n:::\n\n</details>\n\n# Main theorem\n\n<aside>\n\n<br> <br> This is based on [Lemmas 3.3-3.4 @robinson2020introfuncanalysis_book].\n\n</aside>\n\n::: {#thm-normtrichar}\n\n## Characterization of norm triangle inequality\n\nLet $N: V \\to \\mathbb{R_{\\geq 0}}$, be a function satisfying the following two\nproperties[^2].\n\n1.  **Positive definiteness:** For all $\\mathbf{x} \\in V$, if\n    $N(\\mathbf{x}) = 0$ then $\\mathbf{x} = \\mathbf{0}_{V}$.\n2.  **Absolute homogeneity:**\n    $N(\\lambda \\mathbf{x}) = | \\lambda | N(\\mathbf{x})$, for all\n    $\\mathbf{x} \\in V, \\lambda \\in \\mathbb{R}$.\n\nWe then have that: \n\n$$\n    N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})\n    \\text{, for each } \\mathbf{x}, \\mathbf{y} \\in V\n    \\iff \\mathbb{B} := \\{\\mathbf{z} \\in V \\:|\\: N(\\mathbf{z}) \\leq 1 \\}\n    \\text{ is convex}\n$$ {#eq-normtrichar-01}\n\n:::\n\n[^2]: We refer to such an $N: V \\to \\mathbb{R_{\\geq 0}}$ satisfying these\n    properties, as a *norm-like* function.\n\nIn simple terms, the importance of @thm-normtrichar (as captured\nby @eq-normtrichar-01) can be summarized as follows:\n\n> Let $N : V \\to [0, \\infty)$ be a function satisfying positive definiteness and\n> absolute homogeneity. Then $N$ satisfies the triangle inequality if and only\n> if the unit ball induced by $N$, i.e.,\n> $\\mathbb{B} := \\{\\mathbf{z} \\in V \\:|\\: N(\\mathbf{z}) \\leq 1 \\}$, is a convex\n> set.\n\n<details>\n\n<summary>\n\nRemarks\n\n</summary>\n\n::: remark\nIn @thm-normtrichar, we note the following:\n\n1.  The function $N : V \\to \\mathbb{R}_{\\geq 0}$, is a *norm-like* function, and\n    only becomes a valid norm per @def-norm ***once*** we\n    establish the triangle inequality, i.e.,\n    $N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})$.\n2.  To prove the triangle inequality for $N : V \\to \\mathbb{R}_{\\geq 0}$, the\n    necessary condition of @thm-normtrichar to establish is:\n    $$\n     \\mathbb{B} := \\{\\mathbf{z} \\in V \\:|\\: N(\\mathbf{z}) \\leq 1 \\}\n     \\text{ is convex}\n    $${#eq-normtrichar-02} which will imply the triangle inequality for $N$ - huzzah!\n3.  The nice thing is, proving the convexity of $\\mathbb{B}$ can be *much\n    easier* to show than trying to prove the triangle inequality property of $N$\n    directly, as we will soon see.\n4.  **Subtle point:** note that here we had to *assume* that the co-domain of\n    $N$ is non-negative (not $\\mathbb{R}$), i.e.,\n    $N : V \\to \\mathbb{R}_{\\geq 0}$. This is because in a typical norm, which\n    satisfies the triangle inequality, is always shown to be non-negative (see\n    remark below @def-norm for more details). Here we *impose*\n    non-negativity of $N$ as an additional constraint to *establish* the\n    triangle inequality property for $N$. This is not an issue, since one would\n    always first check the non-negativity of a candidate *norm-like* function\n    $N$.\n:::\n\n</details>\n\n# Applications: Minkowski inequalities\n\nBefore getting into the details of the proof, let's just see what Theorem\n@thm-normtrichar can do! We'll consider two related applications taken\nfrom [Lemma 3.6, Example 3.13 @robinson2020introfuncanalysis_book],\nrespectively.\n\n### Application 1: $\\ell_{p}$-norm triangle inequality in $\\mathbb{F}^{n}$\n\n::: {#exm-normtrilpsum}\n\n## Minkowski inequality in finite dimensions\n\nLet us consider $(\\mathbb{F}^{n}, \\mathbb{F})$, where\n$\\mathbb{F} = \\mathbb{R} \\text{ or } \\mathbb{C}$. We then define the *norm-like*\nfunction $N_{\\ell^{p}}: \\mathbb{F}^{n} \\to \\mathbb{R}_{\\geq 0}$:\n\n$$\n    N_{\\ell^{p}}(\\mathbf{x})\n    :=\\left(\\sum_{j=1}^{n}\\left|x_{j}\\right|^{p}\\right)^{1 / p}\n    , \\quad 1 \\leq p<\\infty\n$${#eq-normtrilpsum-01}\n\n:::\n\nOne can show that $N_{\\ell^{p}}$ satisfies positive definiteness and absolute\nhomogeneity. To show that $N_{\\ell^{p}}$ is a norm function we need to prove the\ntriangle inequality. We will use @thm-normtrichar. Let us define\n$\\mathbb{B} := \\{\\mathbf{z} \\in \\mathbb{F}^{n} \\:|\\: N_{\\ell^{p}}(\\mathbf{z}) \\leq 1 \\} = \\{\\mathbf{z} \\in \\mathbb{F}^{n} \\:|\\: N_{\\ell^{p}}^{p}(\\mathbf{z}) \\leq 1 \\}$.\nWe now need to show that $\\mathbb{B}$ is convex. We will need to use the fact\nthat for each $t \\in \\mathbb{R}, t \\mapsto |t|^{p}$ is convex. Let\n$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{B}$, we then have that for\n$\\lambda \\in [0, 1]$:\n\n<aside>\n\n**Note:** $t \\mapsto |t|^{p}$, is convex, for each $t \\in \\mathbb{R}$ and\n$p \\in [1, \\infty)$. **Sketch:** We have, $|t|^{p} =: h(t) := (g \\circ f) (t)$.\nHere, for each $x \\in \\mathbb{R}$, we observe $g : x \\mapsto x^{p}$ is\nincreasing and convex, and $f : x \\mapsto |x|$ is convex. Thus,\n$h(t) := |t|^{p}$ is convex. $\\blacksquare$\n\n</aside>\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\n    N_{\\ell^{p}}^{p}(\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y})\n    & = \\sum_{j=1}^{n}|\\lambda| x_{j}|+(1-\\lambda)| y_{j}||^{p}\n    \\quad\\text{(by definition)} \\\\\n    & \\leq \\sum_{j=1}^{n} \\lambda\\left|x_{j}\\right|^{p}+(1-\\lambda)\\left|y_{j}\\right|^{p}\n    \\quad\\text{(since $t \\mapsto |t|^{p}$ is convex for each $t \\in \\mathbb{R}$)} \\\\\n    & = \\lambda \\sum_{j=1}^{n} \\left|x_{j}\\right|^{p}\n    + (1 - \\lambda) \\sum_{j=1}^{n} \\left|y_{j}\\right|^{p} \\\\\n    & \\leq 1\n    \\quad\\text{(since $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{B}$.)}\n\\end{split}\n\\end{equation}\n```\n\nIt follows that\n$N_{\\ell^{p}}(\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y}) \\leq 1$, and so\n$\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y} \\in \\mathbb{B}$, as required\n$\\blacksquare$.\n\n<aside>\n\nThis triangle inequality is proved in just four lines ðŸ”¥!\n\n</aside>\n\nIn fact, since it $N_{\\ell^{p}}$ satisfies the three conditions for a norm per\n@def-norm we can now denote it using the conventional\n$\\ell_{p}$-norm form, i.e.,\n$\\| \\mathbf{x} \\|_{\\ell^{p}} := N_{\\ell^{p}}(\\mathbf{x})$\n\n### Application 2: $L_{p}$-norm triangle inequality\n\nWe can also similarly prove the triangle inequality norms involving integrals\nefficiently. This is seen in the next example.\n\n<aside>\n\nHere $C([0, 1])$ is the space of continuous functions on the interval $[0, 1]$.\n\n</aside>\n\n::: {#normtrilpint .example name=\"Minkowski inequality in integral form\"}\nLet us consider $(C([0, 1]), \\mathbb{R})$. We then define the *norm-like*\nfunction $N_{L^{p}}: C([0, 1]) \\to \\mathbb{R}_{\\geq 0}$: \n\n$$\n    N_{L^{p}}(\\mathbf{x})\n    :=\\left(\\int_{0}^{1}\\left|f(x)\\right|^{p}\\right)^{1 / p}\n    , \\quad 1 \\leq p<\\infty\n$${#eq-normtrilpint-01}\n\n:::\n\nLet us define\n$\\mathbb{B} := \\{h \\in C([0, 1]) \\:|\\: N_{L^{p}}(h) \\leq 1 \\}  = \\{h \\in C([0, 1]) \\:|\\: N_{L^{p}}^{p}(h) \\leq 1 \\}$.\nWe now need to show that $\\mathbb{B}$ is convex. Let $f, g \\in \\mathbb{B}$, we\nthen have that for $\\lambda \\in [0, 1]$:\n\n<aside>\n\nOnce again, one can show that $N_{\\ell^{p}}$ satisfies positive definiteness and\nabsolute homogeneity.\n\n</aside>\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\n    N_{L^{p}}^{p}(\\lambda f + (1 - \\lambda) g)\n    & = \\int_{0}^{1}|\\lambda f(x) + (1-\\lambda) g(x)|^{p} dx\n    \\quad\\text{(by definition)} \\\\\n    & \\leq \\int_{0}^{1}\\lambda |f(x)|^{p} + (1-\\lambda) |g(x)|^{p} dx\n    \\quad\\text{(since $t \\mapsto |t|^{p}$ is convex for each $t \\in \\mathbb{R}$)} \\\\\n    & = \\lambda \\int_{0}^{1} |f(x)|^{p} dx + (1-\\lambda) \\int_{0}^{1} |g(x)|^{p} dx \\\\\n    & \\leq 1\n    \\quad\\text{(since $f, g \\in \\mathbb{B}$.)}\n\\end{split}\n\\end{equation}\n```\n\nIt follows that $N_{L^{p}}(\\lambda f + (1 - \\lambda) g) \\leq 1$, and so\n$\\lambda f + (1 - \\lambda) g \\in \\mathbb{B}$, as required $\\blacksquare$.\n\nAgain, we can now denote $N_{L^{p}}$ using the conventional $L_{p}$-norm form,\ni.e., $\\| f \\|_{L^{p}} := N_{L^{p}}(f)$.\n\n# Punchline: what did Theorem 1 buy us?\n\nWe just saw that applying @thm-normtrichar enabled us to write **very short\nproofs** of [**Minkowski's\ninequality**](https://en.wikipedia.org/wiki/Minkowski_inequality){.wiki} in\n$\\mathbb{F}^{n}$ and $C([0, 1])$.\n\nTo appreciate this approach, note that proving Minkowski's inequality typically\nrequires one to first prove [**Young's\ninequality**](https://en.wikipedia.org/wiki/Young%27s_inequality_for_products){.wiki} and then\n[**HÃ¶lder's\ninequality**](https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality){.wiki}. Moreover these need to be done separately in\n$\\mathbb{F}^{n}$ and $C([0, 1])$. Using @thm-normtrichar allowed\nus to achieve both of these goals using near identical style of proofs\nðŸŽ‰!\n\n<aside>\n\n**Note:** The proofs of Young's and HÃ¶lder's inequality are beautiful and\nstudying them is also insightful.\n\n</aside>\n\n# Proof of Theorem 1\n\nAssuming $N: V \\to \\mathbb{R_{\\geq 0}}$ satisfies the two properties in Theorem\n@thm-normtrichar, we need to prove both implications in Equation\n@eq-normtrichar-01.\n\n### Proof - easy direction\n\nAssume that $N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})$, for\neach $\\mathbf{x}, \\mathbf{y} \\in V$. Let $\\lambda \\in [0, 1]$ be arbitrary. We\nneed to show that this implies for each $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{B}$\nthat the expression\n$\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y} \\in \\mathbb{B}$ holds. This\nimplies the convexity of $\\mathbb{B}$.\n\n<details>\n\n<summary>\n\nProof\n\n</summary>\n\n::: {#normtrichar-easy .proof}\n($\\implies$) We proceed directly.\n\nAssume that $N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})$, for\neach $\\mathbf{x}, \\mathbf{y} \\in V$. Let $\\lambda \\in [0, 1]$ be arbitrary. We\nneed to show that this implies for each $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{B}$\nthat the expression\n$\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y} \\in \\mathbb{B}$ holds. This\nimplies the convexity of $\\mathbb{B}$.\n\nWe observe that for $\\lambda \\in \\{0, 1\\}$ our required expression is equal to\neither $\\mathbf{x}$ or $\\mathbf{y}$ which are both in $\\mathbb{B}$, by\nassumption. Now fix $\\lambda \\in (0, 1)$. We then note:\n\n\n```{=tex}\n\\begin{equation}\n\\begin{split}\n    N(\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y})\n    & \\leq N(\\lambda \\mathbf{x}) + N((1 - \\lambda) \\mathbf{y})\n    \\quad\\text{($N$ satisfies triangle inequality)} \\\\\n    & = \\lvert \\lambda \\rvert N(\\mathbf{x}) + \\lvert 1 - \\lambda \\rvert N(\\mathbf{y})\n    \\quad\\text{(by absolute homogeneity of $N$)} \\\\\n    & = \\lambda N(\\mathbf{x}) + 1 - \\lambda N(\\mathbf{y})\n    \\quad\\text{(since $\\lambda > 0$)} \\\\\n    & \\leq (\\lambda) (1) + (1 - \\lambda) (1)\n    \\quad\\text{(since $N(\\mathbf{z}) \\leq 1$, for $\\mathbf{z} \\in \\mathbb{B}$)} \\\\\n    & = 1 \\\\\n\\implies \\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y}\n    & \\in \\mathbb{B}\n\\end{split}\n\\end{equation}\n```\n\nWhich implies the convexity of $\\mathbb{B}$, as required. $\\blacksquare$\n:::\n\n</details>\n\n### Proof - interesting direction\n\nAssume $\\mathbb{B}$ is a convex set. We need to show that this implies that\n$N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})$, for each\n$\\mathbf{x}, \\mathbf{y} \\in V$.\n\n<aside>\n\n**Tip:** In linear algebra proofs, it is best to **first** deal with the zero\nvector, $\\mathbf{0}_{V}$, separately.\n\n</aside>\n\n<details>\n\n<summary>\n\nProof\n\n</summary>\n\n::: {#normtrichar-hard .proof}\n($\\impliedby$) We proceed directly.\n\nAssume $\\mathbb{B}$ is a convex set. We need to show that this implies that\n$N(\\mathbf{x} + \\mathbf{y}) \\leq N(\\mathbf{x}) + N(\\mathbf{y})$, for each\n$\\mathbf{x}, \\mathbf{y} \\in V$.\n\nLet $\\mathbf{x}, \\mathbf{y} \\in V$. We will consider four cases.\n\n**Case 1:** Let $\\mathbf{x} = \\mathbf{y} = \\mathbf{0}_{V}$. Then\n$N(\\mathbf{x}) = N(\\mathbf{y}) = N(\\mathbf{0}_{V}) = N(0 \\mathbf{0}_{V})  = |0| N(\\mathbf{0}_{V})= 0$,\nby absolute homogeneity of $N$. Indeed we then have that\n$N(\\mathbf{x} + \\mathbf{y}) = N(\\mathbf{0}_{V}) = 0 = N(\\mathbf{x}) + N(\\mathbf{y})$,\nas required.\n\n**Case 2:** Let\n$\\mathbf{x} = \\mathbf{0}_{V}, \\mathbf{y} \\in V \\setminus \\{\\mathbf{0}_{V}\\}$.\nThen $N(\\mathbf{y}) = 0$, and it follows that\n$N(\\mathbf{x} + \\mathbf{y}) = N(\\mathbf{x} + \\mathbf{0}_{V}) = N(\\mathbf{x})  = N(\\mathbf{x}) + 0 = N(\\mathbf{x}) + N(\\mathbf{y})$,\nas required.\n\n**Case 3:** Let\n$\\mathbf{x} \\in V \\setminus \\{\\mathbf{0}_{V}\\}, \\mathbf{y} = \\mathbf{0}_{V}$.\nSame as **Case 2**, with the roles of $\\mathbf{x}, \\mathbf{y}$ reversed.\n\n**Case 4:** Let $\\mathbf{x}, \\mathbf{y} \\in V \\setminus \\{\\mathbf{0}_{V}\\}$. It\nthen follows that $N(\\mathbf{x}), N(\\mathbf{y}) > 0$, since\n$N(\\mathbf{z}) \\geq 0$, for each $\\mathbf{z} \\in V$, and\n$N(\\mathbf{z}) = 0 \\iff \\mathbf{z} = \\mathbf{0}_{V}$. Moreover we then have that\n$\\lvert N(\\mathbf{x}) \\rvert = N(\\mathbf{x}) > 0$ and\n$\\lvert N(\\mathbf{y}) \\rvert = N(\\mathbf{y}) > 0$. So we can safely divide by\nthese quantities. Let us then define\n$\\tilde{\\mathbf{x}} := \\frac{\\mathbf{x}}{N(\\mathbf{x})}, \\tilde{\\mathbf{y}} := \\frac{\\mathbf{y}}{N(\\mathbf{y})}$.\nWe then have by absolute homogeneity of $N$ that,\n$N(\\tilde{\\mathbf{x}}) := N\\left(\\frac{\\mathbf{x}}{N(\\mathbf{x})}\\right) = \\left \\lvert \\frac{1}{N(\\mathbf{x})} \\right \\rvert N(\\mathbf{x}) = 1 \\implies \\tilde{\\mathbf{x}} \\in \\mathbb{B}$.\nSimilarly $\\tilde{\\mathbf{y}} \\in \\mathbb{B}$. Let us denote\n$\\lambda := \\frac{N(\\mathbf{x})}{N(\\mathbf{x}) + N(\\mathbf{y})} \\in (0, 1)$, and\n$\\mathbf{z} := \\frac{\\mathbf{x} + \\mathbf{y}}{N(\\mathbf{x}) + N(\\mathbf{y})}$.\nWe then have: \\begin{equation}\n\\begin{split}\n    \\mathbf{z}\n    & := \\frac{\\mathbf{x} + \\mathbf{y}}{N(\\mathbf{x}) + N(\\mathbf{y})} \\\\\n    & = \\left(\\frac{N(\\mathbf{x})}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right)\n        \\left(\\frac{\\mathbf{x}}{N(\\mathbf{x})}\\right) +\n        \\left(\\frac{N(\\mathbf{y})}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right)\n        \\left(\\frac{\\mathbf{y}}{N(\\mathbf{y})}\\right) \\\\\n    & = \\left(\\frac{N(\\mathbf{x})}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right)\n        \\tilde{\\mathbf{x}} +\n        \\left(\\frac{N(\\mathbf{y})}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right)\n        \\tilde{\\mathbf{y}} \\\\\n    & = \\lambda \\tilde{\\mathbf{x}} +\n        (1 - \\lambda) \\tilde{\\mathbf{y}} \\\\\n    & \\in \\mathbb{B}\n\\end{split}\n\\end{equation}\n\nBy the assumed convexity of $\\mathbb{B}$. We then have that\n$\\mathbf{z} := \\frac{\\mathbf{x} + \\mathbf{y}}{N(\\mathbf{x}) + N(\\mathbf{y})}  \\in \\mathbb{B} \\implies N(\\mathbf{z}) \\leq 1$.\nWe then observe: \\begin{equation}\n\\begin{split}\n    N(\\mathbf{z})\n    & \\leq 1\n    \\quad\\text{(since $\\mathbf{z} \\in \\mathbb{B}$.)} \\\\\n\\iff N\\left( \\frac{\\mathbf{x} + \\mathbf{y}}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right)\n    & \\leq 1\n    \\quad\\text{(by definition of $\\mathbf{z}$.)} \\\\\n\\iff \\left\\lvert \\frac{1}{N(\\mathbf{x}) + N(\\mathbf{y})}\\right\\rvert\n     N(\\mathbf{x} + \\mathbf{y})\n    & \\leq 1\n    \\quad\\text{(absolute homogeneity of $N$.)} \\\\\n\\iff \\frac{1}{N(\\mathbf{x}) + N(\\mathbf{y})}\n     N(\\mathbf{x} + \\mathbf{y})\n    & \\leq 1\n    \\quad\\text{(since $N(\\mathbf{x}), N(\\mathbf{y}) > 0$.)} \\\\\n\\iff N(\\mathbf{x} + \\mathbf{y})\n    & \\leq N(\\mathbf{x}) + N(\\mathbf{y})\n\\end{split}\n\\end{equation}\n\nAs required. $\\blacksquare$\n:::\n\n</details>\n\n# Recap\n\nIn this article we learned the following about @thm-normtrichar:\n\n-   It gives an alternative way to **characterize** the triangle inequality for\n    *norm-like* functions.\n-   Using this characterization **we can prove the triangle inequality** for\n    such norm-like functions using the convexity of the unit ball induced by\n    such functions.\n-   This is usually easier since we have lots of **tools from convex analysis**\n    to help us prove the convexity of $\\mathbb{B}$.\n-   We saw this in action since @thm-normtrichar enabled us to\n    write **very short proofs** of **Minkowski's inequality** in\n    $\\mathbb{F}^{n}$ and $C([0, 1])$.\n\nIn summary, if you have a norm-like function for which you are trying to\nestablish the triangle inequality, try out @thm-normtrichar\nðŸ’¯!\n\n# Acknowledgements {.appendix}\n\nI thank [Prof. James\nRobinson](https://warwick.ac.uk/fac/sci/maths/people/staff/james_robinson) for\nproviding several technical clarifications on @thm-normtrichar. I thank [Mikhail\nPopov](https://mpopov.com/about/) for creating the [`wikipediapreview` R\npackage](https://bearloga.github.io/wikipediapreview-r/), which enable an easy\ninterface for Wikipedia [Context Cards](https://chimeces.com/context-cards/) in\n`Rmd` files. These Context Cards enable the hover over preview for Wikipedia\narticles. I thank [Jewel Johnson](https://jeweljohnsonj.github.io/jewel_resume/)\nfor providing [this helpful\nguide](https://jeweljohnsonj.github.io/jeweljohnson.github.io/posts/2021-12-18-quality-of-life-modifications-for-your-distill-webistes/#making-the-table-of-contents-more-useful)\nto enable fixed TOC for this article. I thank [Dr. Joel\nNitta](https://www.joelnitta.com/) for providing [these\ninstructions](https://www.joelnitta.com/posts/2021-11-24_using-giscus/) to\nenable us to switch to the [giscus](https://giscus.app/) comments system. Much\nof these distill site improvements were brought to our attention due to the\nexcellent [distillery](https://distillery.rbind.io/tips_and_tricks.html) site\nrun by [Prof. John Paul Helveston](https://www.jhelvy.com/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}